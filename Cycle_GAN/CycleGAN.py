# -*- coding: utf-8 -*-
"""CycleGAN.ipynb

Automatically generated by Colaboratory.


"""

import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from pathlib import Path
from torchvision import datasets , models, transforms
import numpy as np
import matplotlib.image as mpimg
from torchsummary import summary
from sklearn.model_selection import KFold
import os
import random
from torch.utils.data import DataLoader
from torch.autograd import Variable
import torch.utils.data
from scipy.stats import entropy
from torchvision.models.inception import inception_v3
from torch.nn import init 
from tqdm import tqdm 
import torchvision
from torchvision.utils import save_image
import itertools
import torch.utils.data as data
import glob

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
    '.tif', '.TIF', '.tiff', '.TIFF',
]


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)


def make_dataset(dir, max_dataset_size=float("inf")):
    images = []
    assert os.path.isdir(dir) or os.path.islink(dir), '%s is not a valid directory' % dir

    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):
        for fname in fnames:
            if is_image_file(fname):
                path = os.path.join(root, fname)
                images.append(path)
    return images[:min(max_dataset_size, len(images))]


def default_loader(path):
    return Image.open(path).convert('RGB')


class ImageFolder(data.Dataset):

    def __init__(self, root, transform=None, return_paths=False,
                 loader=default_loader, max_dataset_size=float('inf')):
        imgs = make_dataset(root, max_dataset_size=max_dataset_size)
        if len(imgs) == 0:
            raise(RuntimeError("Found 0 images in: " + root + "\n"
                               "Supported image extensions are: " + ",".join(IMG_EXTENSIONS)))

        self.root = root
        self.imgs = imgs
        self.transform = transform
        self.return_paths = return_paths
        self.loader = loader

    def __getitem__(self, index):
        path = self.imgs[index]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        if self.return_paths:
            return img, path
        else:
            return img

    def __len__(self):
        return len(self.imgs)


def conv_norm_relu(in_dim, out_dim, kernel_size, stride = 1, padding=0):
   
    layer = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding),
                          nn.InstanceNorm2d(out_dim), 
                          nn.ReLU(True))
    return layer

def dconv_norm_relu(in_dim, out_dim, kernel_size, stride = 1, padding=0, output_padding=0):
    
    layer = nn.Sequential(nn.ConvTranspose2d(in_dim, out_dim, kernel_size, stride, padding, output_padding),
                          nn.InstanceNorm2d(out_dim), 
                          nn.ReLU(True))
    return layer

class ResidualBlock(nn.Module):
    
    def __init__(self, dim, use_dropout):
        super(ResidualBlock, self).__init__()
        res_block = [nn.ReflectionPad2d(1),
                     conv_norm_relu(dim, dim, kernel_size=3)]
        
        if use_dropout:
            res_block += [nn.Dropout(0.5)]
        res_block += [nn.ReflectionPad2d(1),
                      nn.Conv2d(dim, dim, kernel_size=3, padding=0),
                      nn.InstanceNorm2d(dim)]

        self.res_block = nn.Sequential(*res_block)

    def forward(self, x):
        return x + self.res_block(x)


class Generator(nn.Module):
    
    def __init__(self, input_nc=3, output_nc=3, filters=64, use_dropout=True, n_blocks=2):
        super(Generator, self).__init__()
        
        
# Down Sample

        model = [nn.ReflectionPad2d(3),
                 conv_norm_relu(input_nc   , filters * 1, 7),
                 conv_norm_relu(filters * 1, filters * 2, 3, 2, 1),
                 conv_norm_relu(filters * 2, filters * 4, 3, 2, 1)]
        
# 頸脖層

        for i in range(n_blocks):
            model += [ResidualBlock(filters * 4, use_dropout)]
       
# Up Sample

        model += [dconv_norm_relu(filters * 4, filters * 2, 3, 2, 1, 1),
                  dconv_norm_relu(filters * 2, filters * 1, 3, 2, 1, 1),
                  nn.ReflectionPad2d(3),
                  nn.Conv2d(filters, output_nc, 7),
                  nn.Tanh()]

        self.model = nn.Sequential(*model)    

    def forward(self, x):
        return self.model(x)

G = Generator()
if torch.cuda.is_available():
  G.cuda()
summary(G, (3,256,256))



def conv_norm_leakyrelu(in_dim, out_dim, kernel_size, stride = 1, padding=0, output_padding=0):

    
    layer = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding),
                          nn.InstanceNorm2d(out_dim), 
                          nn.LeakyReLU(0.2,True))
    return layer

class Discriminator(nn.Module):
    
    def __init__(self, input_nc=3, filters=64, n_layer = 3):
        super(Discriminator, self).__init__()

        model = [
            nn.Conv2d(input_nc, filters, kernel_size=1, stride=1, padding=0),
            nn.LeakyReLU(0.2, True)]

        for i in range(1, n_layer):
            n_filters_prev = 2**(i-1)
            n_filters = 2**i
            model += [conv_norm_leakyrelu(filters * n_filters_prev , filters * n_filters, kernel_size=4,
                                           stride=2, padding=1)]
        

        n_filters_prev = 2**(n_layer-1)
        n_filters = 2**n_layer
        model += [conv_norm_leakyrelu(filters * n_filters_prev , filters * n_filters, kernel_size=4,
                                           stride=1, padding=1)]

        model += [nn.Conv2d(filters * n_filters, 1, kernel_size=4, stride=1, padding=1)]
        
        self.model = nn.Sequential(*model)

    def forward(self, input):
        return self.model(input)
    
D = Discriminator()
if torch.cuda.is_available():
  D.cuda()

summary(D, (3,256,256))

###### initial ######


def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.normal_(m.weight.data, 0.0, 0.02)

###### basic parameters ######

        
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
batch_size = 4
epochs = 100
decay_epoch = 10
lr = 2e-3
log_freq = 100


############ define Loss function ############


MSE = nn.MSELoss()
L1 = nn.L1Loss()

############ define optimizer ############


class LambdaLR():
    def __init__(self, epochs, offset, decay_epoch):
        self.epochs = epochs
        self.offset = offset
        self.decay_epoch = decay_epoch

    def step(self, epoch):
        return 1.0 - max(0, epoch + self.offset - self.decay_epoch)/(self.epochs - self.decay_epoch)

# To store 50 generated image in a pool and sample from it when it is full

# Shrivastava et al’s strategy

class ReplayBuffer:
    def __init__(self, max_size=50):
        assert (max_size > 0), "Empty buffer or trying to create a black hole. Be careful."
        self.max_size = max_size
        self.data = []

    def push_and_pop(self, data):
        to_return = []
        for element in data.data:
            element = torch.unsqueeze(element, 0)
            if len(self.data) < self.max_size:
                self.data.append(element)
                to_return.append(element)
            else:
                if random.uniform(0, 1) > 0.5:
                    i = random.randint(0, self.max_size - 1)
                    to_return.append(self.data[i].clone())
                    self.data[i] = element
                else:
                    to_return.append(element)
        return torch.cat(to_return)
    
fake_A_sample = ReplayBuffer()
fake_B_sample = ReplayBuffer()

############ Define Model ############

use_cuda = torch.cuda.is_available()
if not use_cuda:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...') 

G_A2B = Generator()
G_B2A = Generator()
D_A = Discriminator()
D_B = Discriminator()

G_A2B.apply(weights_init_normal)
G_B2A.apply(weights_init_normal)
D_A.apply(weights_init_normal)
D_B.apply(weights_init_normal)


optim_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=(0.5, 0.999))
optim_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))
optim_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))
        
lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optim_G,lr_lambda=LambdaLR(epochs, 0, decay_epoch).step)
lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optim_D_A, lr_lambda=LambdaLR(epochs, 0, decay_epoch).step)
lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optim_D_B, lr_lambda=LambdaLR(epochs, 0, decay_epoch).step)



ep=[]
genloss=[]
disloss=[]
totalloss=[]

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
    '.tif', '.TIF', '.tiff', '.TIFF',
]


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)


def make_dataset(dir, max_dataset_size=float("inf")):
    images = []
    assert os.path.isdir(dir) or os.path.islink(dir), '%s is not a valid directory' % dir

    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):
        for fname in fnames:
            if is_image_file(fname):
                path = os.path.join(root, fname)
                images.append(path)
    return images[:min(max_dataset_size, len(images))]


def default_loader(path):
    return Image.open(path).convert('RGB')


class ImageFolder(data.Dataset):

    def __init__(self, root, transform=None, return_paths=False,
                 loader=default_loader, max_dataset_size=float('inf')):
        imgs = make_dataset(root, max_dataset_size=max_dataset_size)
        if len(imgs) == 0:
            raise(RuntimeError("Found 0 images in: " + root + "\n"
                               "Supported image extensions are: " + ",".join(IMG_EXTENSIONS)))

        self.root = root
        self.imgs = imgs
        self.transform = transform
        self.return_paths = return_paths
        self.loader = loader

    def __getitem__(self, index):
        path = self.imgs[index]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        if self.return_paths:
            return img, path
        else:
            return img

    def __len__(self):
        return len(self.imgs)


ep=[]
genloss=[]
disloss=[]
totalloss=[]


mean , std = torch.tensor([0.5, 0.5, 0.5]),torch.tensor([0.5, 0.5, 0.5])

def denormalize(image):
  image = transforms.Normalize(-mean/std,1/std)(image) #denormalize
  image = image.permute(1,2,0) #Changing from 3x224x224 to 224x224x3
  image = torch.clamp(image,0,1)
  return image

# helper function to un-normalize and display an image
def imshow(img):
    img = denormalize(img) 
    plt.imshow(img.detach().numpy())

def back(image):
  image = transforms.Normalize(-mean/std,1/std)(image) #denormalize
  #image = image.permute(1,2,0) #Changing from 3x224x224 to 224x224x3
  #image = torch.clamp(image,0,1)
  return image

train_transform = transforms.Compose(
            [transforms.RandomHorizontalFlip(),
             transforms.Resize((286, 286)),
             transforms.RandomCrop((256, 256)),
             transforms.ToTensor(),
             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])

path_train="MONET"

photoinput = 'gan-getting-started/photo_jpg'
monetinput = 'gan-getting-started/monet_jpg'
photo=ImageFolder(photoinput, transform=train_transform)
monet=ImageFolder(monetinput, transform=train_transform)


a_loader = torch.utils.data.DataLoader(monet, batch_size=batch_size, shuffle=True, num_workers=4)
b_loader = torch.utils.data.DataLoader(photo, batch_size=batch_size, shuffle=True, num_workers=4)

use_cuda = torch.cuda.is_available()
if use_cuda:
  G_A2B.cuda()
  G_B2A.cuda()
  D_A.cuda()
  D_B.cuda()

def draw(images,use_cuda):
  if use_cuda:
    images=images.cuda()
  # plot the images in the batch, along with the corresponding labels
  fake_image_A = G_B2A(images)
  fig = plt.figure(figsize=(10 ,4))
  # display 20 images
  for idx in np.arange(batch_size):
    ax = fig.add_subplot(2, int(batch_size), idx+1, xticks=[], yticks=[])
    imshow(images[idx].cpu())
    ax.set_title("Original")
    ax = fig.add_subplot(2, int(batch_size), idx+1+batch_size, xticks=[], yticks=[])
    imshow(fake_image_A[idx].cpu())
    ax.set_title("Monet Style")
  plt.show()
  


dataiter = iter(b_loader)
images, labels = dataiter.next()
draw(images,use_cuda)

""" train """

i=0
for epoch in range(1, epochs+1):
  print("Epoch : {}".format(epoch))
  draw(images,use_cuda)
  for i, (a_real, b_real) in enumerate(zip(a_loader, b_loader)):

    # get batch size data
    real_image_A = a_real
    real_image_B = b_real
    if use_cuda:
      #0 for monet 1 for photo 
      real_image_A = real_image_A.cuda()
      real_image_B = real_image_B.cuda()

    # real data label is 1, fake data label is 0.
    real_label = torch.full((batch_size, 1,62,62), 1, device=device, dtype=torch.float32)
    fake_label = torch.full((batch_size, 1,62,62), 0, device=device, dtype=torch.float32)

    ##############################################
    # (1) Update G network: Generators A2B and B2A
    ##############################################

    # Set G_A and G_B's gradients to zero
    optim_G.zero_grad()

    # Identity loss
    # G_B2A should generate A style pic
    # so G_B2A(A) should look like A
    identity_image_A = G_B2A(real_image_A) 
    loss_identity_A = L1(identity_image_A, real_image_A) * 5.0 #L1 loss
    
    # G_A2B should generate B style pic
    # so G_A2B(B) should look like B
    identity_image_B = G_A2B(real_image_B)
    loss_identity_B = L1(identity_image_B, real_image_B) * 5.0

    # GAN loss = adversarial loss
    # GAN loss D_A(G_A(A))
    fake_image_A = G_B2A(real_image_B)
    fake_output_A = D_A(fake_image_A)
    loss_GAN_B2A = MSE(fake_output_A, real_label)
    
    
    # GAN loss D_B(G_B(B))
    fake_image_B = G_A2B(real_image_A)
    fake_output_B = D_B(fake_image_B)
    loss_GAN_A2B = MSE(fake_output_B, real_label)

    # Cycle loss
    recovered_image_A = G_B2A(fake_image_B)
    loss_cycle_ABA = L1(recovered_image_A, real_image_A) * 10.0

    recovered_image_B = G_A2B(fake_image_A)
    loss_cycle_BAB = L1(recovered_image_B, real_image_B) * 10.0

    # Combined loss and calculate gradients
    G_error = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB
    
    # Calculate gradients for G_A and G_B
    G_error.backward()
    # Update G_A and G_B's weights
    optim_G.step()

    ##############################################
    # (2) Update D network: Discriminator A
    ##############################################

    # Set D_A gradients to zero
    optim_D_A.zero_grad()

    # Real A image loss
    real_output_A = D_A(real_image_A)
    errD_real_A = MSE(real_output_A, real_label)

    # Fake A image loss
    fake_image_A = fake_A_sample.push_and_pop(fake_image_A)
    
    fake_output_A = D_A(fake_image_A.detach())
    errD_fake_A = MSE(fake_output_A, fake_label)

    # Combined loss and calculate gradients
    D_A_error = (errD_real_A + errD_fake_A) / 2.0
    
    # Calculate gradients for D_A
    D_A_error.backward()
    # Update D_A weights
    optim_D_A.step()

    ##############################################
    # (3) Update D network: Discriminator B
    ##############################################

    # Set D_B gradients to zero
    optim_D_B.zero_grad()

    # Real B image loss
    real_output_B = D_B(real_image_B)
    errD_real_B = MSE(real_output_B, real_label)

    # Fake B image loss
    fake_image_B = fake_B_sample.push_and_pop(fake_image_B)
    fake_output_B = D_B(fake_image_B.detach())
    errD_fake_B = MSE(fake_output_B, fake_label)

    # Combined loss and calculate gradients
    D_B_error = (errD_real_B + errD_fake_B) / 2

    # Calculate gradients for D_B
    D_B_error.backward()
    # Update D_B weights
    optim_D_B.step()
    i+=1
    genloss.append(G_error)
    disloss.append( D_B_error + D_A_error)
    totalloss.append( D_B_error + D_A_error + G_error)
    ep.append(i)
  
  print('\n')
  print("Epoch: (%3d) (%5d/%5d) | Gen Loss:%.5f | Dis Loss:%.5f" % 
                                    (epoch, i + 1, min(len(a_loader), len(b_loader)),
                                                    G_error,D_A_error+D_B_error))
#   # Save the Model
#   torch.save(G_A2B.state_dict(),"0624_G_A2B/G_A2B_epoch_{}.pth".format(epoch))
#   torch.save(G_B2A.state_dict(),"0624_G_B2A/G_B2A_epoch_{}.pth".format(epoch))
#   torch.save(D_A.state_dict(),"0624_D_A/D_A_epoch_{}.pth".format(epoch))
#   torch.save(D_B.state_dict(),"0624_D_B/D_B_epoch_{}.pth".format(epoch))

  # Update learning rates
  lr_scheduler_G.step()
  lr_scheduler_D_A.step()
  lr_scheduler_D_B.step()

with open('genloss.json', 'w') as f:
    json.dump(genloss, f)
with open('disloss.json', 'w') as f:
    json.dump(disloss, f)
with open('totalloss.json', 'w') as f:
    json.dump(totalloss, f)
plt.title('Gen loss!')
plt.plot(ep,genloss, color=(255/255,191/255,0/255))
plt.show()

plt.title('Dis loss!')
plt.plot(ep,disloss, color=(13/255,191/255,140/255))
plt.show()

plt.title('Total loss!')
plt.plot(ep,totalloss, color=(70/255,130/255,180/255))
plt.show()

""" test """
try:
  os.makedirs("cyclegan_resnet_photos")
# 檔案已存在的例外處理
except FileExistsError:
   pass
# try:
#   os.makedirs("data/CycleGAN_pic")
# # 檔案已存在的例外處理
# except FileExistsError:
#    pass
idx=0
for i,b_real in enumerate(b_loader):
  b_real=b_real[0]
  if use_cuda:
    b_real=b_real.cuda()
  b_monet=G_B2A(b_real)
  b_monet = back(b_monet)
  b_real = back(b_real)
  if len(b_real)!=4 or len(b_monet)!=4:
    print(i)
    print(len(b_real))
  for i in range(min(len(b_real),len(b_monet))):
    save_image(b_monet[i], 'cyclegan_resnet_photos/{}.jpg'.format(idx))
    idx+=1







